{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaconda environment setup modules.\n",
    "\n",
    "```\n",
    "module load cuda10.2/toolkit/10.2.89\n",
    "module load CUDA/10.2.89-GCC-6.4.0-2.28\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retreive Amazon review data.\n",
    "\n",
    "TODO: Save large files to disk and train model against them in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_URL = 'http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Electronics.json.gz'\n",
    "#METADATA_URL = 'http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_Electronics.json.gz'\n",
    "DATA_URL = 'http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Gift_Cards.json.gz'\n",
    "METADATA_URL = 'http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_Gift_Cards.json.gz'\n",
    "SAMPLE_SIZE = 60000\n",
    "\n",
    "MODEL_SAVE_PATH = '/data/user/cren1/bert2gpt'\n",
    "MODEL_TEMP_PATH = '/data/user/cren1/temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import gzip\n",
    "from urllib.request import urlopen\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_data(url):\n",
    "    data = []\n",
    "    with gzip.open(urlopen(url)) as f:\n",
    "        for l in f:\n",
    "            data.append(json.loads(l.strip()))\n",
    "    return data\n",
    "\n",
    "data = get_data(DATA_URL)\n",
    "metadata = get_data(METADATA_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create product ASIN mapping to link products and their reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Remove HTML tags from text\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<.*?>', '', text)\n",
    "\n",
    "product_dataset = {}\n",
    "for product in metadata:\n",
    "    asin = product['asin']\n",
    "    category = ' '.join(product['category'])\n",
    "    title = product['title']\n",
    "    description = ' '.join(product['description'])\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    filtered_description = remove_html_tags(description)\n",
    "    \n",
    "    product_dataset[asin] = {\n",
    "        'category': category,\n",
    "        'title': title,\n",
    "        'description': filtered_description\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.bias', 'h.0.crossattention.masked_bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.bias', 'h.0.ln_cross_attn.weight', 'h.0.ln_cross_attn.bias', 'h.1.crossattention.bias', 'h.1.crossattention.masked_bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.bias', 'h.1.ln_cross_attn.weight', 'h.1.ln_cross_attn.bias', 'h.2.crossattention.bias', 'h.2.crossattention.masked_bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.c_proj.bias', 'h.2.ln_cross_attn.weight', 'h.2.ln_cross_attn.bias', 'h.3.crossattention.bias', 'h.3.crossattention.masked_bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.bias', 'h.3.ln_cross_attn.weight', 'h.3.ln_cross_attn.bias', 'h.4.crossattention.bias', 'h.4.crossattention.masked_bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.bias', 'h.4.ln_cross_attn.weight', 'h.4.ln_cross_attn.bias', 'h.5.crossattention.bias', 'h.5.crossattention.masked_bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.bias', 'h.5.ln_cross_attn.weight', 'h.5.ln_cross_attn.bias', 'h.6.crossattention.bias', 'h.6.crossattention.masked_bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.bias', 'h.6.ln_cross_attn.weight', 'h.6.ln_cross_attn.bias', 'h.7.crossattention.bias', 'h.7.crossattention.masked_bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.c_proj.bias', 'h.7.ln_cross_attn.weight', 'h.7.ln_cross_attn.bias', 'h.8.crossattention.bias', 'h.8.crossattention.masked_bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.c_proj.bias', 'h.8.ln_cross_attn.weight', 'h.8.ln_cross_attn.bias', 'h.9.crossattention.bias', 'h.9.crossattention.masked_bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.c_proj.bias', 'h.9.ln_cross_attn.weight', 'h.9.ln_cross_attn.bias', 'h.10.crossattention.bias', 'h.10.crossattention.masked_bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.c_proj.bias', 'h.10.ln_cross_attn.weight', 'h.10.ln_cross_attn.bias', 'h.11.crossattention.bias', 'h.11.crossattention.masked_bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.c_proj.bias', 'h.11.ln_cross_attn.weight', 'h.11.ln_cross_attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, GPT2Tokenizer, EncoderDecoderModel\n",
    "\n",
    "RATING_TOKEN = '[RAT]'\n",
    "CATEGORY_TOKEN = '[CAT]'\n",
    "TITLE_TOKEN = '[TTL]'\n",
    "DESCRIPTION_TOKEN = '[DES]'\n",
    "LABEL_MASK_TOKEN_ID = -100\n",
    "\n",
    "ENCODER_MAX_LENGTH = 512\n",
    "DECODER_MAX_LENGTH = 128\n",
    "\n",
    "encoder_tokenizer = BertTokenizer(\n",
    "    'vocab.txt',\n",
    "    additional_special_tokens=[\n",
    "        RATING_TOKEN,\n",
    "        CATEGORY_TOKEN,\n",
    "        TITLE_TOKEN,\n",
    "        DESCRIPTION_TOKEN,\n",
    "    ],\n",
    ")\n",
    "encoder_tokenizer.bos_token = encoder_tokenizer.cls_token\n",
    "encoder_tokenizer.eos_token = encoder_tokenizer.sep_token\n",
    "\n",
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "    return outputs\n",
    "\n",
    "GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "decoder_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "decoder_tokenizer.pad_token = decoder_tokenizer.unk_token\n",
    "\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')\n",
    "model.config.vocab_size = model.config.encoder.vocab_size\n",
    "\n",
    "model.config.decoder_start_token_id = decoder_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = decoder_tokenizer.eos_token_id\n",
    "model.config.pad_token_id = decoder_tokenizer.eos_token_id\n",
    "\n",
    "model.config.max_length = 142\n",
    "model.config.min_length = 56\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.early_stopping = True\n",
    "model.length_penalty = 2.0\n",
    "model.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map data to model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244471ea5cdf46be8f4974c9519496ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=147194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# map data to input\n",
    "def generate_input_sequence(rating, category, title, description):\n",
    "    rating_sequence = '{} {}'.format(RATING_TOKEN, rating)\n",
    "    category_sequence = '{} {}'.format(CATEGORY_TOKEN, category)\n",
    "    title_sequence = '{} {}'.format(TITLE_TOKEN, title)\n",
    "    description_sequence = '{} {}'.format(DESCRIPTION_TOKEN, description)\n",
    "    return ' '.join([rating_sequence, category_sequence, title_sequence, description_sequence])\n",
    "\n",
    "# tokenize input for encoder\n",
    "def preprocess_encoder_input(sequence):\n",
    "    tokenized_input = encoder_tokenizer.tokenize(sequence)\n",
    "    encoded_input = encoder_tokenizer.convert_tokens_to_ids(tokenized_input)\n",
    "    prepared_input = encoder_tokenizer.prepare_for_model(\n",
    "        encoded_input,\n",
    "        max_length=ENCODER_MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    return prepared_input['input_ids'], prepared_input['attention_mask']\n",
    "\n",
    "# tokenize input for decoder\n",
    "def preprocess_decoder_input(sequence):\n",
    "    tokenized_input = decoder_tokenizer.tokenize(sequence)\n",
    "    encoded_input = decoder_tokenizer.convert_tokens_to_ids(tokenized_input)\n",
    "    prepared_input = decoder_tokenizer.prepare_for_model(\n",
    "        encoded_input,\n",
    "        max_length=DECODER_MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    return prepared_input['input_ids'], prepared_input['attention_mask']\n",
    "\n",
    "model_inputs = []\n",
    "for review in tqdm(data):\n",
    "    product_asin = review['asin']\n",
    "    review_rating = review['overall']\n",
    "    review_text = review.get('reviewText')\n",
    "    \n",
    "    product = product_dataset.get(product_asin)\n",
    "    \n",
    "    # filter reviews with missing information\n",
    "    has_text_review = review_text is not None\n",
    "    product_exists = product is not None\n",
    "    product_has_description = product_exists and product['description'] is not []\n",
    "    \n",
    "    if has_text_review and product_has_description:\n",
    "        product_combined = generate_input_sequence(\n",
    "            review_rating,\n",
    "            product['category'],\n",
    "            product['title'],\n",
    "            product['description'],\n",
    "        )\n",
    "        \n",
    "        input_ids, attention_mask = preprocess_encoder_input(product_combined)\n",
    "        decoder_input_ids, decoder_attention_mask = preprocess_decoder_input(review_text)\n",
    "        \n",
    "        # mask pad tokens in label\n",
    "        labels = []\n",
    "        for index in decoder_input_ids:\n",
    "            label = index\n",
    "            if index is decoder_tokenizer.pad_token_id:\n",
    "                label = LABEL_MASK_TOKEN_ID\n",
    "            labels.append(label)\n",
    "    \n",
    "        model_inputs.append({\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'decoder_input_ids': decoder_input_ids,\n",
    "            'decoder_attention_mask': decoder_attention_mask,\n",
    "            'labels': labels,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and testing inputs. Initialize datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets==1.0.2 in /home/cren1/.local/lib/python3.8/site-packages (1.0.2)\n",
      "Requirement already satisfied: pandas in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from datasets==1.0.2) (1.0.5)\n",
      "Requirement already satisfied: filelock in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from datasets==1.0.2) (3.0.12)\n",
      "Requirement already satisfied: dill in /home/cren1/.local/lib/python3.8/site-packages (from datasets==1.0.2) (0.3.3)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/cren1/.local/lib/python3.8/site-packages (from datasets==1.0.2) (2.0.0)\n",
      "Requirement already satisfied: xxhash in /home/cren1/.local/lib/python3.8/site-packages (from datasets==1.0.2) (2.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from datasets==1.0.2) (2.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from datasets==1.0.2) (4.47.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from datasets==1.0.2) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from pandas->datasets==1.0.2) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from pandas->datasets==1.0.2) (2.8.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.0.2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.0.2) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.0.2) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
      "Requirement already satisfied: six>=1.5 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas->datasets==1.0.2) (1.15.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/cren1/.local/lib/python3.8/site-packages (3.5.1)\n",
      "Requirement already satisfied: numpy in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: protobuf in /home/cren1/.local/lib/python3.8/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: requests in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: filelock in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /home/cren1/.local/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /home/cren1/.local/lib/python3.8/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: packaging in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tokenizers==0.9.3 in /home/cren1/.local/lib/python3.8/site-packages (from transformers) (0.9.3)\n",
      "Requirement already satisfied: six>=1.9 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from protobuf->transformers) (1.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: joblib in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: click in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets==1.0.2\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "data_frame = pd.DataFrame.from_records(model_inputs)\n",
    "dataset = Dataset.from_pandas(data_frame).select(range(SAMPLE_SIZE))\n",
    "dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "dataset_dict = dataset.train_test_split()\n",
    "train_dataset = dataset_dict['train']\n",
    "test_dataset = dataset_dict['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install seq2seq trainer prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rouge_score in /home/cren1/.local/lib/python3.8/site-packages (0.0.4)\n",
      "Requirement already satisfied: absl-py in /home/cren1/.local/lib/python3.8/site-packages (from rouge_score) (0.11.0)\n",
      "Requirement already satisfied: numpy in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from rouge_score) (1.18.5)\n",
      "Requirement already satisfied: nltk in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from rouge_score) (3.5)\n",
      "Requirement already satisfied: six>=1.14.0 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from rouge_score) (1.15.0)\n",
      "Requirement already satisfied: click in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from nltk->rouge_score) (7.1.2)\n",
      "Requirement already satisfied: joblib in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from nltk->rouge_score) (0.16.0)\n",
      "Requirement already satisfied: regex in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from nltk->rouge_score) (2020.6.8)\n",
      "Requirement already satisfied: tqdm in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from nltk->rouge_score) (4.47.0)\n",
      "--2020-11-16 07:43:46--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/seq2seq/seq2seq_trainer.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.192.133, 151.101.0.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.192.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9810 (9.6K) [text/plain]\n",
      "Saving to: ‘seq2seq_trainer.py’\n",
      "\n",
      "100%[======================================>] 9,810       --.-K/s   in 0s      \n",
      "\n",
      "2020-11-16 07:43:46 (48.0 MB/s) - ‘seq2seq_trainer.py’ saved [9810/9810]\n",
      "\n",
      "--2020-11-16 07:43:47--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/seq2seq/seq2seq_training_args.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.64.133, 151.101.128.133, 151.101.192.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.64.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2071 (2.0K) [text/plain]\n",
      "Saving to: ‘seq2seq_training_args.py’\n",
      "\n",
      "100%[======================================>] 2,071       --.-K/s   in 0s      \n",
      "\n",
      "2020-11-16 07:43:47 (20.6 MB/s) - ‘seq2seq_training_args.py’ saved [2071/2071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n",
    "\n",
    "!rm seq2seq_trainer.py\n",
    "!rm seq2seq_training_args.py\n",
    "!wget https://raw.githubusercontent.com/huggingface/transformers/v3.5.1/examples/seq2seq/seq2seq_trainer.py\n",
    "!wget https://raw.githubusercontent.com/huggingface/transformers/v3.5.1/examples/seq2seq/seq2seq_training_args.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(outputs):\n",
    "    predictions_ids = outputs.predictions\n",
    "    labels_ids = outputs.label_ids\n",
    "\n",
    "    predictions = decoder_tokenizer.batch_decode(predictions_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == LABEL_MASK_TOKEN_ID] = decoder_tokenizer.eos_token_id\n",
    "    labels = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(\n",
    "        predictions=predictions,\n",
    "        references=labels,\n",
    "        rouge_types=[\"rouge2\"]\n",
    "    )[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cren1/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729062494/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/data/user/cren1/.conda/envs/NLP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='15001' max='33750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15001/33750 8:05:42 < 10:07:08, 0.51 it/s, Epoch 1.33/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.127674</td>\n",
       "      <td>3.061293</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='671' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 671/3750 57:12 < 4:22:53, 0.20 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9e2a6639b84b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m )\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_epoch_stop\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch)\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset)\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0meval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_eval_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m         output = self.prediction_loop(\n\u001b[0m\u001b[1;32m   1309\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only)\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1417\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1418\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m                 \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS 662/project/seq2seq_trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meach\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \"\"\"\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         gen_kwargs = {\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m                 \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_past\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from seq2seq_trainer import Seq2SeqTrainer\n",
    "from seq2seq_training_args import Seq2SeqTrainingArguments\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    output_dir=os.path.join(MODEL_TEMP_PATH, 'results'),\n",
    "    overwrite_output_dir=True,\n",
    "    #save_steps=10,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    #eval_steps=4,\n",
    "    eval_steps=7500,\n",
    "    logging_dir=os.path.join(MODEL_TEMP_PATH, 'runs'),\n",
    "    #logging_steps=2,\n",
    "    logging_steps=1000,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    fp16=True,\n",
    "    warmup_steps=2000,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoderModel.from_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict from input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "product = list(product_dataset.values())[0]\n",
    "product_category = product['category']\n",
    "product_title = product['title']\n",
    "product_description = product['description']\n",
    "product_combined = generate_input_sequence(5, product_category, product_title, product_description)\n",
    "\n",
    "input_ids, _ = preprocess_encoder_input(product_combined)\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    decoder_start_token_id=model.config.decoder.pad_token_id,\n",
    "    temperature=1.3,\n",
    "    top_k=9,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORY: Gift Cards Gift Cards\n",
      "TITLE: Serendipity 3 $100.00 Gift Card\n",
      "DESCRIPTION: Gift card for the purchase of goods or services at Serendipity 3 in New York City only. Not valid for online purchases. Statements regarding dietary supplements have not been evaluated by the FDA and are not intended to diagnose, treat, cure, or prevent any disease or health condition.\n",
      "PREPROCESSED_INPUT: [CLS] [RAT] 5 [CAT] gift cards gift cards [TTL] serendipity 3 $ 100. 00 gift card [DES] gift card for the purchase of goods or services at serendipity 3 in new york city only. not valid for online purchases. statements regarding dietary supplements have not been evaluated by the fda and are not intended to diagnose, treat, cure, or prevent any disease or health condition. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "OUTPUT: <|endoftext|>I love the convenience of being able to print out a gift card and have it delivered directly from Amazon.  It's so easy!\n",
      "Thanks for making this so convenient!! :) I will definitely use these again in future gifts!!! : ) (D)RKWJH<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print('CATEGORY:', product_category)\n",
    "print('TITLE:', product_title)\n",
    "print('DESCRIPTION:', product_description)\n",
    "\n",
    "print('PREPROCESSED_INPUT:', encoder_tokenizer.decode(input_ids.squeeze(0)))\n",
    "print('OUTPUT:', decoder_tokenizer.decode(output_ids.squeeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-NLP]",
   "language": "python",
   "name": "conda-env-.conda-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
