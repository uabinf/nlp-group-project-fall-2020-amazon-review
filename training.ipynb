{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uy9tFxepFvf1"
   },
   "source": [
    "Retreive Amazon review data.\n",
    "\n",
    "TODO: Save large files to disk and train model against them in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0enZtx3ROg3",
    "outputId": "1e0e8903-3930-4170-a773-fff74c42ea81"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-zt-SlvGC8C",
    "outputId": "f70fec93-540a-44dd-b9c1-efd3bbcc2d4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# https://stackoverflow.com/a/59331412/9295513\\n# same\\n!wget -c https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\\n!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\\n!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\\n# update 1\\n!conda install -q -y --prefix /usr/local python=3.6 ujson\\n# update 2\\nimport sys\\nsys.path.append('/usr/local/lib/python3.6/site-packages')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install anaconda\n",
    "#!pip install transformers\n",
    "\"\"\"\n",
    "# https://stackoverflow.com/a/59331412/9295513\n",
    "# same\n",
    "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
    "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
    "!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n",
    "# update 1\n",
    "!conda install -q -y --prefix /usr/local python=3.6 ujson\n",
    "# update 2\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.6/site-packages')\n",
    "\"\"\"\n",
    "#!conda env create --file \"/content/drive/My Drive/Colab-Notebooks/environment.yml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o-ZDZFsAFvf2"
   },
   "outputs": [],
   "source": [
    "#https://github.com/bhargaviparanjape/clickbait\n",
    "DATA_URL = 'https://github.com/bhargaviparanjape/clickbait/raw/master/dataset/clickbait_data.gz'\n",
    "SAMPLE_SIZE = 60000\n",
    "\n",
    "MODEL_SAVE_PATH = '/data/user/jprob/bert2gpt'#'/content/drive/My Drive/Colab-Notebooks/bert2gpt'\n",
    "MODEL_TEMP_PATH = '/data/user/jprob/temp'#'/content/drive/My Drive/Colab-Notebooks/temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jNtV9Vs1Fvf2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "from urllib.request import urlopen\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_data(url):\n",
    "    data = []\n",
    "    with gzip.open(urlopen(url)) as f:\n",
    "        for l in f:\n",
    "          if l != b'\\n':\n",
    "            data.append(str(l)[2:].rstrip(r\"\\n'\"))\n",
    "    return data\n",
    "\n",
    "data = get_data(DATA_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7_d9oONFvf2"
   },
   "source": [
    "Create product ASIN mapping to link products and their reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "R8xv3BT5Fvf2",
    "outputId": "8aaea20a-39af-4bd6-9f4b-60d1735e4f89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nproduct_dataset = {}\\nfor product in metadata:\\n    asin = product['asin']\\n    category = ' '.join(product['category'])\\n    title = product['title']\\n    description = ' '.join(product['description'])\\n    \\n    # Remove HTML tags\\n    filtered_description = remove_html_tags(description)\\n    \\n    product_dataset[asin] = {\\n        'category': category,\\n        'title': title,\\n        'description': filtered_description\\n    }\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Remove HTML tags from text\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<.*?>', '', text)\n",
    "\"\"\"\n",
    "product_dataset = {}\n",
    "for product in metadata:\n",
    "    asin = product['asin']\n",
    "    category = ' '.join(product['category'])\n",
    "    title = product['title']\n",
    "    description = ' '.join(product['description'])\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    filtered_description = remove_html_tags(description)\n",
    "    \n",
    "    product_dataset[asin] = {\n",
    "        'category': category,\n",
    "        'title': title,\n",
    "        'description': filtered_description\n",
    "    }\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOVO3W2dFvf3"
   },
   "source": [
    "Initialize tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJ8rbab4Fvf3",
    "outputId": "1e5b3aa3-03c1-4285-8e57-560dad2c8d67",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.bias', 'h.0.crossattention.masked_bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.bias', 'h.0.ln_cross_attn.weight', 'h.0.ln_cross_attn.bias', 'h.1.crossattention.bias', 'h.1.crossattention.masked_bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.bias', 'h.1.ln_cross_attn.weight', 'h.1.ln_cross_attn.bias', 'h.2.crossattention.bias', 'h.2.crossattention.masked_bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.c_proj.bias', 'h.2.ln_cross_attn.weight', 'h.2.ln_cross_attn.bias', 'h.3.crossattention.bias', 'h.3.crossattention.masked_bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.bias', 'h.3.ln_cross_attn.weight', 'h.3.ln_cross_attn.bias', 'h.4.crossattention.bias', 'h.4.crossattention.masked_bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.bias', 'h.4.ln_cross_attn.weight', 'h.4.ln_cross_attn.bias', 'h.5.crossattention.bias', 'h.5.crossattention.masked_bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.bias', 'h.5.ln_cross_attn.weight', 'h.5.ln_cross_attn.bias', 'h.6.crossattention.bias', 'h.6.crossattention.masked_bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.bias', 'h.6.ln_cross_attn.weight', 'h.6.ln_cross_attn.bias', 'h.7.crossattention.bias', 'h.7.crossattention.masked_bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.c_proj.bias', 'h.7.ln_cross_attn.weight', 'h.7.ln_cross_attn.bias', 'h.8.crossattention.bias', 'h.8.crossattention.masked_bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.c_proj.bias', 'h.8.ln_cross_attn.weight', 'h.8.ln_cross_attn.bias', 'h.9.crossattention.bias', 'h.9.crossattention.masked_bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.c_proj.bias', 'h.9.ln_cross_attn.weight', 'h.9.ln_cross_attn.bias', 'h.10.crossattention.bias', 'h.10.crossattention.masked_bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.c_proj.bias', 'h.10.ln_cross_attn.weight', 'h.10.ln_cross_attn.bias', 'h.11.crossattention.bias', 'h.11.crossattention.masked_bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.c_proj.bias', 'h.11.ln_cross_attn.weight', 'h.11.ln_cross_attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, GPT2Tokenizer, EncoderDecoderModel\n",
    "\n",
    "RATING_TOKEN = '[RAT]'\n",
    "CATEGORY_TOKEN = '[CAT]'\n",
    "TITLE_TOKEN = '[TTL]'\n",
    "DESCRIPTION_TOKEN = '[DES]'\n",
    "LABEL_MASK_TOKEN_ID = -100\n",
    "\n",
    "ENCODER_MAX_LENGTH = 512\n",
    "DECODER_MAX_LENGTH = 128\n",
    "\n",
    "encoder_tokenizer = BertTokenizer(\n",
    "    'vocab.txt'\n",
    "    #'/content/drive/My Drive/Colab-Notebooks/vocab.txt'#,\n",
    "    #additional_special_tokens=[RATING_TOKEN,CATEGORY_TOKEN,TITLE_TOKEN,DESCRIPTION_TOKEN,],\n",
    ")\n",
    "encoder_tokenizer.bos_token = encoder_tokenizer.cls_token\n",
    "encoder_tokenizer.eos_token = encoder_tokenizer.sep_token\n",
    "\n",
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "    return outputs\n",
    "\n",
    "GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "decoder_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "decoder_tokenizer.pad_token = decoder_tokenizer.unk_token\n",
    "\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')\n",
    "model.config.vocab_size = model.config.encoder.vocab_size\n",
    "\n",
    "model.config.decoder_start_token_id = decoder_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = decoder_tokenizer.eos_token_id\n",
    "model.config.pad_token_id = decoder_tokenizer.eos_token_id\n",
    "\n",
    "model.config.max_length = 142\n",
    "model.config.min_length = 56\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.early_stopping = True\n",
    "model.length_penalty = 2.0\n",
    "model.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eNX7-fCFvf4"
   },
   "source": [
    "Map data to model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65,
     "referenced_widgets": [
      "aa8142622f46424e908223dd8e5fb2d3",
      "e8640797cf664b3eb0f1dad59ac4498b",
      "0c901fdd0a264615a8ef71633f23f5fe",
      "f94265cfdd1d4079b429dd0f7f40fb44",
      "7d31159958d34ec7943598b8eb23fbe0",
      "34809ffa2683439b86c8cf14919b72ef",
      "ca869822cb424c5cb1742b973d53b470",
      "e1bb80f43db348728804c3ad3a6c57de"
     ]
    },
    "id": "rwl9iyxOFvf4",
    "outputId": "db5968ed-f58a-4c4a-ba7b-6038f3eb7084"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7138e51456754c75a476363d73e24039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# map data to input\n",
    "def generate_input_sequence(rating, category, title, description):\n",
    "    rating_sequence = '{} {}'.format(RATING_TOKEN, rating)\n",
    "    category_sequence = '{} {}'.format(CATEGORY_TOKEN, category)\n",
    "    title_sequence = '{} {}'.format(TITLE_TOKEN, title)\n",
    "    description_sequence = '{} {}'.format(DESCRIPTION_TOKEN, description)\n",
    "    return ' '.join([rating_sequence, category_sequence, title_sequence, description_sequence])\n",
    "\n",
    "# tokenize input for encoder\n",
    "def preprocess_encoder_input(sequence):\n",
    "    tokenized_input = encoder_tokenizer.tokenize(sequence)\n",
    "    encoded_input = encoder_tokenizer.convert_tokens_to_ids(tokenized_input)\n",
    "    prepared_input = encoder_tokenizer.prepare_for_model(\n",
    "        encoded_input,\n",
    "        max_length=ENCODER_MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    return prepared_input['input_ids'], prepared_input['attention_mask']\n",
    "\n",
    "# tokenize input for decoder\n",
    "def preprocess_decoder_input(sequence):\n",
    "    tokenized_input = decoder_tokenizer.tokenize(sequence)\n",
    "    encoded_input = decoder_tokenizer.convert_tokens_to_ids(tokenized_input)\n",
    "    prepared_input = decoder_tokenizer.prepare_for_model(\n",
    "        encoded_input,\n",
    "        max_length=DECODER_MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    return prepared_input['input_ids'], prepared_input['attention_mask']\n",
    "\n",
    "model_inputs = []\n",
    "for review in tqdm(data):\n",
    "    #product_asin = review['asin']\n",
    "    #review_rating = review['overall']\n",
    "    #review_text = review.get('reviewText')\n",
    "    \n",
    "    #product = product_dataset.get(product_asin)\n",
    "    \n",
    "    # filter reviews with missing information\n",
    "    has_text_review = True #review_text is not None\n",
    "    product_exists = True #product is not None\n",
    "    product_has_description = True #product_exists and product['description'] is not []\n",
    "    \n",
    "\n",
    "    if has_text_review and product_has_description:\n",
    "        \n",
    "        product_combined = generate_input_sequence(\n",
    "            \" \", #review_rating,\n",
    "            \" \", #product['category'],\n",
    "            \" \", #product['title'],\n",
    "            \" \", #product['description'],\n",
    "        )\n",
    "        \n",
    "        input_ids, attention_mask = preprocess_encoder_input(product_combined)\n",
    "        decoder_input_ids, decoder_attention_mask = preprocess_decoder_input(review) #review_text\n",
    "        \n",
    "        # mask pad tokens in label\n",
    "        labels = []\n",
    "        for index in decoder_input_ids:\n",
    "            label = index\n",
    "            if index is decoder_tokenizer.pad_token_id:\n",
    "                label = LABEL_MASK_TOKEN_ID\n",
    "            labels.append(label)\n",
    "    \n",
    "        model_inputs.append({\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'decoder_input_ids': decoder_input_ids,\n",
    "            'decoder_attention_mask': decoder_attention_mask,\n",
    "            'labels': labels,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJbRwF1uFvf6"
   },
   "source": [
    "Split into training and testing inputs. Initialize datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KgeYAGihFvf7",
    "outputId": "d4828b83-5887-467f-fbbf-adc9d0aa8df6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets==1.0.2 in ./.local/lib/python3.8/site-packages (1.0.2)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.8/site-packages (from datasets==1.0.2) (2.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from datasets==1.0.2) (2.24.0)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in ./.local/lib/python3.8/site-packages (from datasets==1.0.2) (2.0.0)\n",
      "Requirement already satisfied: filelock in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from datasets==1.0.2) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from datasets==1.0.2) (4.47.0)\n",
      "Requirement already satisfied: dill in ./.local/lib/python3.8/site-packages (from datasets==1.0.2) (0.3.3)\n",
      "Requirement already satisfied: pandas in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from datasets==1.0.2) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from datasets==1.0.2) (1.18.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.0.2) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.0.2) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.0.2) (3.0.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from pandas->datasets==1.0.2) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from pandas->datasets==1.0.2) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas->datasets==1.0.2) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -mpip install datasets==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3wMy-BbkFvf7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "data_frame = pd.DataFrame.from_records(model_inputs)\n",
    "dataset = Dataset.from_pandas(data_frame).select(range(SAMPLE_SIZE))\n",
    "dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "dataset_dict = dataset.train_test_split()\n",
    "train_dataset = dataset_dict['train']\n",
    "test_dataset = dataset_dict['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43wUMH4VFvf7"
   },
   "source": [
    "Install seq2seq trainer prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ApR_3c4mFvf7",
    "outputId": "0a537a87-3605-44be-a463-048198b9d851",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rouge_score in ./.local/lib/python3.8/site-packages (0.0.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from rouge_score) (1.15.0)\n",
      "Requirement already satisfied: absl-py in ./.local/lib/python3.8/site-packages (from rouge_score) (0.11.0)\n",
      "Requirement already satisfied: nltk in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from rouge_score) (3.5)\n",
      "Requirement already satisfied: numpy in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from rouge_score) (1.18.5)\n",
      "Requirement already satisfied: joblib in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from nltk->rouge_score) (0.16.0)\n",
      "Requirement already satisfied: regex in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from nltk->rouge_score) (2020.6.8)\n",
      "Requirement already satisfied: click in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from nltk->rouge_score) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /data/rc/apps/rc/software/Anaconda3/2020.07/lib/python3.8/site-packages (from nltk->rouge_score) (4.47.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install rouge_score\n",
    "!{sys.executable} -mpip install rouge_score\n",
    "# !rm seq2seq_trainer.py\n",
    "# !rm seq2seq_training_args.py\n",
    "# !wget https://raw.githubusercontent.com/huggingface/transformers/v3.5.1/examples/seq2seq/seq2seq_trainer.py\n",
    "# !wget https://raw.githubusercontent.com/huggingface/transformers/v3.5.1/examples/seq2seq/seq2seq_training_args.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRXmAtceFvf7"
   },
   "source": [
    "Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65,
     "referenced_widgets": [
      "2ba3fd25d35e4e3280947dd489ee6a06",
      "eb68d725f55740dea5d3844bf0525acc",
      "0c65b9a318dd4f3d809ac538a347af31",
      "ed8d29a6f0fe4198bdb534150064f85f",
      "1b682ab4ab1c4591ab0511f0c8a70699",
      "bb73f8cba2644f2c9c6938eb1f8025d0",
      "92b0478b273645bca4893c165530130c",
      "130ecab1173442a0ba1c5ccd6a20f80d"
     ]
    },
    "id": "CsWgDEfuFvf7",
    "outputId": "cc486c66-8da0-45ad-ab02-21f4fd939eef"
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(outputs):\n",
    "    predictions_ids = outputs.predictions\n",
    "    labels_ids = outputs.label_ids\n",
    "\n",
    "    predictions = decoder_tokenizer.batch_decode(predictions_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == LABEL_MASK_TOKEN_ID] = decoder_tokenizer.eos_token_id\n",
    "    labels = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(\n",
    "        predictions=predictions,\n",
    "        references=labels,\n",
    "        rouge_types=[\"rouge2\"]\n",
    "    )[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itxL-QIvo9ea"
   },
   "source": [
    "The next cell is taken from the transformers examples directory. Importing was broken for it and it alone for some reason, so I copied it and removed things until it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jrAre_ks3o-e"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "\n",
    "from transformers import PreTrainedModel, Trainer, logging\n",
    "from transformers.file_utils import is_torch_tpu_available\n",
    "from transformers.models.fsmt.configuration_fsmt import FSMTConfig\n",
    "from transformers.optimization import (\n",
    "    Adafactor,\n",
    "    AdamW,\n",
    "    get_constant_schedule,\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_polynomial_decay_schedule_with_warmup,\n",
    ")\n",
    "from transformers.trainer_pt_utils import get_tpu_sampler\n",
    "#from transformers.training_args import ParallelMode\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "arg_to_scheduler = {\n",
    "    \"linear\": get_linear_schedule_with_warmup,\n",
    "    \"cosine\": get_cosine_schedule_with_warmup,\n",
    "    \"cosine_w_restarts\": get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "    \"polynomial\": get_polynomial_decay_schedule_with_warmup,\n",
    "    \"constant\": get_constant_schedule,\n",
    "    \"constant_w_warmup\": get_constant_schedule_with_warmup,\n",
    "}\n",
    "\n",
    "\n",
    "class Seq2SeqTrainer(Trainer):\n",
    "    def __init__(self, config=None, data_args=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        if config is None:\n",
    "            assert isinstance(\n",
    "                self.model, PreTrainedModel\n",
    "            ), f\"If no `config` is passed the model to be trained has to be of type `PreTrainedModel`, but is {self.model.__class__}\"\n",
    "            self.config = self._actual_model(self.model).config\n",
    "        else:\n",
    "            self.config = config\n",
    "\n",
    "        self.data_args = data_args\n",
    "        self.vocab_size = self.config.tgt_vocab_size if isinstance(self.config, FSMTConfig) else self.config.vocab_size\n",
    "\n",
    "        if self.args.label_smoothing != 0 or (self.data_args is not None and self.data_args.ignore_pad_token_for_loss):\n",
    "            assert (\n",
    "                self.config.pad_token_id is not None\n",
    "            ), \"Make sure that `config.pad_token_id` is correcly defined when ignoring `pad_token` for loss calculation or doing label smoothing.\"\n",
    "\n",
    "        if self.config.pad_token_id is None and self.config.eos_token_id is not None:\n",
    "            logger.warn(\n",
    "                f\"The `config.pad_token_id` is `None`. Using `config.eos_token_id` = {self.config.eos_token_id} for padding..\"\n",
    "            )\n",
    "\n",
    "        if self.args.label_smoothing == 0:\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=self.config.pad_token_id)\n",
    "        else:\n",
    "            # dynamically import label_smoothed_nll_loss\n",
    "            from utils import label_smoothed_nll_loss\n",
    "\n",
    "            self.loss_fn = label_smoothed_nll_loss\n",
    "\n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        \"\"\"\n",
    "        Setup the optimizer and the learning rate scheduler.\n",
    "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
    "        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
    "        \"\"\"\n",
    "        if self.optimizer is None:\n",
    "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay\": self.args.weight_decay,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "            ]\n",
    "            if self.args.adafactor:\n",
    "                self.optimizer = Adafactor(\n",
    "                    optimizer_grouped_parameters,\n",
    "                    lr=self.args.learning_rate,\n",
    "                    scale_parameter=False,\n",
    "                    relative_step=False,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                self.optimizer = AdamW(\n",
    "                    optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon\n",
    "                )\n",
    "\n",
    "        if self.lr_scheduler is None:\n",
    "            self.lr_scheduler = self._get_lr_scheduler(num_training_steps)\n",
    "        else:  # ignoring --lr_scheduler\n",
    "            logger.warn(\"scheduler is passed to `Seq2SeqTrainer`, `--lr_scheduler` arg is ignored.\")\n",
    "\n",
    "    def _get_lr_scheduler(self, num_training_steps):\n",
    "        schedule_func = arg_to_scheduler[self.args.lr_scheduler]\n",
    "        if self.args.lr_scheduler == \"constant\":\n",
    "            scheduler = schedule_func(self.optimizer)\n",
    "        elif self.args.lr_scheduler == \"constant_w_warmup\":\n",
    "            scheduler = schedule_func(self.optimizer, num_warmup_steps=self.args.warmup_steps)\n",
    "        else:\n",
    "            scheduler = schedule_func(\n",
    "                self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps\n",
    "            )\n",
    "        return scheduler\n",
    "\n",
    "    def _get_train_sampler(self) -> Optional[torch.utils.data.sampler.Sampler]:\n",
    "        if isinstance(self.train_dataset, torch.utils.data.IterableDataset):\n",
    "            return None\n",
    "        elif is_torch_tpu_available():\n",
    "            return get_tpu_sampler(self.train_dataset)\n",
    "        else:\n",
    "            if self.args.sortish_sampler:\n",
    "                self.train_dataset.make_sortish_sampler(\n",
    "                    self.args.per_device_train_batch_size,\n",
    "                    distributed=(self.args.parallel_mode == ParallelMode.DISTRIBUTED),\n",
    "                )\n",
    "\n",
    "            return (\n",
    "                RandomSampler(self.train_dataset)\n",
    "                if self.args.local_rank == -1\n",
    "                else DistributedSampler(self.train_dataset)\n",
    "            )\n",
    "\n",
    "    def _compute_loss(self, model, inputs, labels):\n",
    "        if self.args.label_smoothing == 0:\n",
    "            if self.data_args is not None and self.data_args.ignore_pad_token_for_loss:\n",
    "                # force training to ignore pad token\n",
    "                logits = model(**inputs, use_cache=False)[0]\n",
    "                loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "            else:\n",
    "                # compute usual loss via models\n",
    "                loss, logits = model(**inputs, labels=labels, use_cache=False)[:2]\n",
    "        else:\n",
    "            # compute label smoothed loss\n",
    "            logits = model(**inputs, use_cache=False)[0]\n",
    "            lprobs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "            loss, _ = self.loss_fn(lprobs, labels, self.args.label_smoothing, ignore_index=self.config.pad_token_id)\n",
    "        return loss, logits\n",
    "\n",
    "    def compute_loss(self, model, inputs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        loss, _ = self._compute_loss(model, inputs, labels)\n",
    "        return loss\n",
    "\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Perform an evaluation step on :obj:`model` using obj:`inputs`.\n",
    "        Subclass and override to inject custom behavior.\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to evaluate.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "            prediction_loss_only (:obj:`bool`):\n",
    "                Whether or not to return the loss only.\n",
    "        Return:\n",
    "            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "            A tuple with the loss, logits and labels (each being optional).\n",
    "        \"\"\"\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": self.data_args.val_max_target_length\n",
    "            if self.data_args is not None\n",
    "            else self.config.max_length,\n",
    "            \"num_beams\": self.data_args.eval_beams if self.data_args is not None else self.config.num_beams,\n",
    "        }\n",
    "\n",
    "        if self.args.predict_with_generate and not self.args.prediction_loss_only:\n",
    "            generated_tokens = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                **gen_kwargs,\n",
    "            )\n",
    "            # in case the batch is shorter than max length, the output should be padded\n",
    "            if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n",
    "                generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n",
    "\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        with torch.no_grad():\n",
    "            # compute loss on predict data\n",
    "            loss, logits = self._compute_loss(model, inputs, labels)\n",
    "\n",
    "        loss = loss.mean().detach()\n",
    "        if self.args.prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "\n",
    "        logits = generated_tokens if self.args.predict_with_generate else logits\n",
    "\n",
    "        if labels.shape[-1] < gen_kwargs[\"max_length\"]:\n",
    "            labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n",
    "\n",
    "        return (loss, logits, labels)\n",
    "\n",
    "    def _pad_tensors_to_max_len(self, tensor, max_length):\n",
    "        # If PAD token is not defined at least EOS token has to be defined\n",
    "        pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else self.config.eos_token_id\n",
    "\n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\n",
    "                f\"Make sure that either `config.pad_token_id` or `config.eos_token_id` is defined if tensor has to be padded to `max_length`={max_length}\"\n",
    "            )\n",
    "\n",
    "        padded_tensor = pad_token_id * torch.ones(\n",
    "            (tensor.shape[0], max_length), dtype=tensor.dtype, device=tensor.device\n",
    "        )\n",
    "        padded_tensor[:, : tensor.shape[-1]] = tensor\n",
    "        return padded_tensor\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "#from seq2seq_trainer import arg_to_scheduler\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Seq2SeqTrainingArguments(TrainingArguments):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        label_smoothing (:obj:`float`, `optional`, defaults to 0):\n",
    "            The label smoothing epsilon to apply (if not zero).\n",
    "        sortish_sampler (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether to SortishSamler or not. It sorts the inputs according to lenghts in-order to minimizing the padding size.\n",
    "        predict_with_generate (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether to use generate to calculate generative metrics (ROUGE, BLEU).\n",
    "    \"\"\"\n",
    "\n",
    "    label_smoothing: Optional[float] = field(\n",
    "        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (if not zero).\"}\n",
    "    )\n",
    "    sortish_sampler: bool = field(default=False, metadata={\"help\": \"Whether to SortishSamler or not.\"})\n",
    "    predict_with_generate: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n",
    "    )\n",
    "    adafactor: bool = field(default=False, metadata={\"help\": \"whether to use adafactor\"})\n",
    "    encoder_layerdrop: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Encoder layer dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    decoder_layerdrop: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Decoder layer dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    dropout: Optional[float] = field(default=None, metadata={\"help\": \"Dropout probability. Goes into model.config.\"})\n",
    "    attention_dropout: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Attention dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    lr_scheduler: Optional[str] = field(\n",
    "        default=\"linear\",\n",
    "        metadata={\"help\": f\"Which lr scheduler to use. Selected in {sorted(arg_to_scheduler.keys())}\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "IrvyX5NqFvf7",
    "outputId": "0126d68b-ea49-4055-ec1b-b427eff5c1ca",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-94369f7a4fdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0mCan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterable\u001b[0m \u001b[0mof\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \"\"\"\n\u001b[0;32m-> 1066\u001b[0;31m         return self._getitem(\n\u001b[0m\u001b[1;32m   1067\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m             \u001b[0mformat_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, format_type, format_columns, output_all_columns, format_kwargs)\u001b[0m\n\u001b[1;32m    943\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes_mapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_types_mapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m                     outputs = self._unnest(\n\u001b[0m\u001b[1;32m    946\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes_mapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_types_mapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m                     )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_unnest\u001b[0;34m(py_dict)\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpy_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpy_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    output_dir=os.path.join(MODEL_TEMP_PATH, 'results'),\n",
    "    overwrite_output_dir=True,\n",
    "    #save_steps=10,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    #eval_steps=4,\n",
    "    eval_steps=7500,\n",
    "    logging_dir=os.path.join(MODEL_TEMP_PATH, 'runs'),\n",
    "    #logging_steps=2,\n",
    "    logging_steps=1000,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    fp16=True,\n",
    "    warmup_steps=2000,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YXDawD2Fvf8"
   },
   "source": [
    "Save model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49OzXK9iFvf8"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rB5z9R4EFvf8"
   },
   "source": [
    "Load model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ps0E9QGoFvf8"
   },
   "outputs": [],
   "source": [
    "model = EncoderDecoderModel.from_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rwPkWLYFvf8"
   },
   "source": [
    "Predict from input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suePi9ZfFvf8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "product = list(product_dataset.values())[0]\n",
    "product_category = product['category']\n",
    "product_title = product['title']\n",
    "product_description = product['description']\n",
    "product_combined = generate_input_sequence(5, product_category, product_title, product_description)\n",
    "\n",
    "input_ids, _ = preprocess_encoder_input(product_combined)\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    decoder_start_token_id=model.config.decoder.pad_token_id,\n",
    "    temperature=1.3,\n",
    "    top_k=9,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1HKV0fnFvf9"
   },
   "outputs": [],
   "source": [
    "print('CATEGORY:', product_category)\n",
    "print('TITLE:', product_title)\n",
    "print('DESCRIPTION:', product_description)\n",
    "\n",
    "print('PREPROCESSED_INPUT:', encoder_tokenizer.decode(input_ids.squeeze(0)))\n",
    "print('OUTPUT:', decoder_tokenizer.decode(output_ids.squeeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWJb5hSXFvf9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c65b9a318dd4f3d809ac538a347af31": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb73f8cba2644f2c9c6938eb1f8025d0",
      "max": 1717,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1b682ab4ab1c4591ab0511f0c8a70699",
      "value": 1717
     }
    },
    "0c901fdd0a264615a8ef71633f23f5fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34809ffa2683439b86c8cf14919b72ef",
      "max": 15999,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7d31159958d34ec7943598b8eb23fbe0",
      "value": 15999
     }
    },
    "130ecab1173442a0ba1c5ccd6a20f80d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b682ab4ab1c4591ab0511f0c8a70699": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2ba3fd25d35e4e3280947dd489ee6a06": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0c65b9a318dd4f3d809ac538a347af31",
       "IPY_MODEL_ed8d29a6f0fe4198bdb534150064f85f"
      ],
      "layout": "IPY_MODEL_eb68d725f55740dea5d3844bf0525acc"
     }
    },
    "34809ffa2683439b86c8cf14919b72ef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d31159958d34ec7943598b8eb23fbe0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "92b0478b273645bca4893c165530130c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa8142622f46424e908223dd8e5fb2d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0c901fdd0a264615a8ef71633f23f5fe",
       "IPY_MODEL_f94265cfdd1d4079b429dd0f7f40fb44"
      ],
      "layout": "IPY_MODEL_e8640797cf664b3eb0f1dad59ac4498b"
     }
    },
    "bb73f8cba2644f2c9c6938eb1f8025d0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca869822cb424c5cb1742b973d53b470": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1bb80f43db348728804c3ad3a6c57de": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8640797cf664b3eb0f1dad59ac4498b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb68d725f55740dea5d3844bf0525acc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed8d29a6f0fe4198bdb534150064f85f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_130ecab1173442a0ba1c5ccd6a20f80d",
      "placeholder": "​",
      "style": "IPY_MODEL_92b0478b273645bca4893c165530130c",
      "value": " 4.20k/? [00:00&lt;00:00, 13.6kB/s]"
     }
    },
    "f94265cfdd1d4079b429dd0f7f40fb44": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1bb80f43db348728804c3ad3a6c57de",
      "placeholder": "​",
      "style": "IPY_MODEL_ca869822cb424c5cb1742b973d53b470",
      "value": " 15999/15999 [00:27&lt;00:00, 577.73it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
