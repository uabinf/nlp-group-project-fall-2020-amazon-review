{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d0fa507e20dc45c6a9f710af204ff053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0a1194e867524960962c7ad62be14dab",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a3f330447b4b4ac6943e984803389b0c",
              "IPY_MODEL_f2bb53734aa2494ab7342eeff624d814"
            ]
          }
        },
        "0a1194e867524960962c7ad62be14dab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3f330447b4b4ac6943e984803389b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_622bb6e22453446b9d23fcc047c5d350",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 15999,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 15999,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb50c2164a194d1abf75cb4aba79557e"
          }
        },
        "f2bb53734aa2494ab7342eeff624d814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8ae94de70bfe4ba3bca86071f6d58ef3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 15999/15999 [00:12&lt;00:00, 1329.55it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7fc38b477af94798b096b819a155e2d4"
          }
        },
        "622bb6e22453446b9d23fcc047c5d350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb50c2164a194d1abf75cb4aba79557e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ae94de70bfe4ba3bca86071f6d58ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7fc38b477af94798b096b819a155e2d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy9tFxepFvf1"
      },
      "source": [
        "Retreive Amazon review data.\n",
        "\n",
        "TODO: Save large files to disk and train model against them in batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0enZtx3ROg3",
        "outputId": "d750bb3e-a18b-4b53-db8b-52cadbb41bb5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "C-zt-SlvGC8C",
        "outputId": "56322460-a907-4a00-de3c-06d84a51a288"
      },
      "source": [
        "#!pip install anaconda\n",
        "!pip install transformers\n",
        "\"\"\"\n",
        "# https://stackoverflow.com/a/59331412/9295513\n",
        "# same\n",
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n",
        "# update 1\n",
        "!conda install -q -y --prefix /usr/local python=3.6 ujson\n",
        "# update 2\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages')\n",
        "\"\"\"\n",
        "#!conda env create --file \"/content/drive/My Drive/Colab-Notebooks/environment.yml\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# https://stackoverflow.com/a/59331412/9295513\\n# same\\n!wget -c https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\\n!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\\n!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\\n# update 1\\n!conda install -q -y --prefix /usr/local python=3.6 ujson\\n# update 2\\nimport sys\\nsys.path.append('/usr/local/lib/python3.6/site-packages')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-ZDZFsAFvf2"
      },
      "source": [
        "#https://github.com/bhargaviparanjape/clickbait\n",
        "DATA_URL = 'https://github.com/bhargaviparanjape/clickbait/raw/master/dataset/clickbait_data.gz'\n",
        "SAMPLE_SIZE = 60000\n",
        "\n",
        "MODEL_SAVE_PATH = '/data/user/jprob/bert2gpt'#'/content/drive/My Drive/Colab-Notebooks/bert2gpt'\n",
        "MODEL_TEMP_PATH = '/data/user/jprob/temp'#'/content/drive/My Drive/Colab-Notebooks/temp'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNtV9Vs1Fvf2"
      },
      "source": [
        "import os\n",
        "import gzip\n",
        "from urllib.request import urlopen\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"training_data\")\n",
        "except FileExistsError:\n",
        "  pass # The directory already exists.\n",
        "\n",
        "def get_data(url):\n",
        "    data = []\n",
        "    with gzip.open(urlopen(url)) as f:\n",
        "        for l in f:\n",
        "          if l != b'\\n':\n",
        "            data.append(str(l)[2:].rstrip(r\"\\n'\"))\n",
        "    return data\n",
        "\n",
        "u_input = input(\"First time?[y] for yes, anything else for no\")\n",
        "\n",
        "if u_input.lower() == \"y\":\n",
        "  data = get_data(DATA_URL)\n",
        "\n",
        "  # https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
        "  n = 20\n",
        "  data_batches = [data[i:i + n] for i in range(0, len(data), n)]\n",
        "\n",
        "  for c, i in enumerate(data_batches):\n",
        "    with open(f\"training_data/{c}.txt\", \"w\") as f:\n",
        "      f.write(str(i))\n",
        "else:\n",
        "  data = []\n",
        "  for i in os.listdir(\"training_data\"):\n",
        "    with open(\"training_data/\" + i, \"r\") as f:\n",
        "      f.readline()\n",
        "      for l in f:\n",
        "        data.append(json.loads(l.strip()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7_d9oONFvf2"
      },
      "source": [
        "Create product ASIN mapping to link products and their reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "R8xv3BT5Fvf2",
        "outputId": "eac628ba-d947-4545-95dc-a511a08eab6f"
      },
      "source": [
        "import re\n",
        "\n",
        "# Remove HTML tags from text\n",
        "def remove_html_tags(text):\n",
        "    return re.sub(r'<.*?>', '', text)\n",
        "\"\"\"\n",
        "product_dataset = {}\n",
        "for product in metadata:\n",
        "    asin = product['asin']\n",
        "    category = ' '.join(product['category'])\n",
        "    title = product['title']\n",
        "    description = ' '.join(product['description'])\n",
        "    \n",
        "    # Remove HTML tags\n",
        "    filtered_description = remove_html_tags(description)\n",
        "    \n",
        "    product_dataset[asin] = {\n",
        "        'category': category,\n",
        "        'title': title,\n",
        "        'description': filtered_description\n",
        "    }\"\"\"\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nproduct_dataset = {}\\nfor product in metadata:\\n    asin = product['asin']\\n    category = ' '.join(product['category'])\\n    title = product['title']\\n    description = ' '.join(product['description'])\\n    \\n    # Remove HTML tags\\n    filtered_description = remove_html_tags(description)\\n    \\n    product_dataset[asin] = {\\n        'category': category,\\n        'title': title,\\n        'description': filtered_description\\n    }\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOVO3W2dFvf3"
      },
      "source": [
        "Initialize tokenizer and model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ8rbab4Fvf3",
        "scrolled": true,
        "outputId": "70a53c57-374a-4947-f922-2628e3f5b2ea"
      },
      "source": [
        "from transformers import BertTokenizer, GPT2Tokenizer, EncoderDecoderModel\n",
        "\n",
        "RATING_TOKEN = '[RAT]'\n",
        "CATEGORY_TOKEN = '[CAT]'\n",
        "TITLE_TOKEN = '[TTL]'\n",
        "DESCRIPTION_TOKEN = '[DES]'\n",
        "LABEL_MASK_TOKEN_ID = -100\n",
        "\n",
        "ENCODER_MAX_LENGTH = 512\n",
        "DECODER_MAX_LENGTH = 128\n",
        "\n",
        "encoder_tokenizer = BertTokenizer(\n",
        "    '/content/drive/My Drive/Colab-Notebooks/vocab.txt'#,\n",
        "    #additional_special_tokens=[RATING_TOKEN,CATEGORY_TOKEN,TITLE_TOKEN,DESCRIPTION_TOKEN,],\n",
        ")\n",
        "encoder_tokenizer.bos_token = encoder_tokenizer.cls_token\n",
        "encoder_tokenizer.eos_token = encoder_tokenizer.sep_token\n",
        "\n",
        "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
        "    return outputs\n",
        "\n",
        "GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
        "decoder_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "decoder_tokenizer.pad_token = decoder_tokenizer.unk_token\n",
        "\n",
        "#https://huggingface.co/transformers/model_doc/bert.html below number is default vocab size, vocab_size=30522\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')\n",
        "model.config.vocab_size = 30522 - 1 #model.config.encoder.vocab_size #should be vocab_size - 1, find the vocab size; vocab size of model should be same as tokenizer\n",
        "\n",
        "model.config.decoder_start_token_id = decoder_tokenizer.bos_token_id\n",
        "model.config.eos_token_id = decoder_tokenizer.eos_token_id\n",
        "model.config.pad_token_id = decoder_tokenizer.eos_token_id\n",
        "\n",
        "model.config.max_length = 142\n",
        "model.config.min_length = 56\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.early_stopping = True\n",
        "model.length_penalty = 2.0\n",
        "model.num_beams = 4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.bias', 'h.0.crossattention.masked_bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.bias', 'h.0.ln_cross_attn.weight', 'h.0.ln_cross_attn.bias', 'h.1.crossattention.bias', 'h.1.crossattention.masked_bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.bias', 'h.1.ln_cross_attn.weight', 'h.1.ln_cross_attn.bias', 'h.2.crossattention.bias', 'h.2.crossattention.masked_bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.c_proj.bias', 'h.2.ln_cross_attn.weight', 'h.2.ln_cross_attn.bias', 'h.3.crossattention.bias', 'h.3.crossattention.masked_bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.bias', 'h.3.ln_cross_attn.weight', 'h.3.ln_cross_attn.bias', 'h.4.crossattention.bias', 'h.4.crossattention.masked_bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.bias', 'h.4.ln_cross_attn.weight', 'h.4.ln_cross_attn.bias', 'h.5.crossattention.bias', 'h.5.crossattention.masked_bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.bias', 'h.5.ln_cross_attn.weight', 'h.5.ln_cross_attn.bias', 'h.6.crossattention.bias', 'h.6.crossattention.masked_bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.bias', 'h.6.ln_cross_attn.weight', 'h.6.ln_cross_attn.bias', 'h.7.crossattention.bias', 'h.7.crossattention.masked_bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.c_proj.bias', 'h.7.ln_cross_attn.weight', 'h.7.ln_cross_attn.bias', 'h.8.crossattention.bias', 'h.8.crossattention.masked_bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.c_proj.bias', 'h.8.ln_cross_attn.weight', 'h.8.ln_cross_attn.bias', 'h.9.crossattention.bias', 'h.9.crossattention.masked_bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.c_proj.bias', 'h.9.ln_cross_attn.weight', 'h.9.ln_cross_attn.bias', 'h.10.crossattention.bias', 'h.10.crossattention.masked_bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.c_proj.bias', 'h.10.ln_cross_attn.weight', 'h.10.ln_cross_attn.bias', 'h.11.crossattention.bias', 'h.11.crossattention.masked_bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.c_proj.bias', 'h.11.ln_cross_attn.weight', 'h.11.ln_cross_attn.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eNX7-fCFvf4"
      },
      "source": [
        "Map data to model inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "d0fa507e20dc45c6a9f710af204ff053",
            "0a1194e867524960962c7ad62be14dab",
            "a3f330447b4b4ac6943e984803389b0c",
            "f2bb53734aa2494ab7342eeff624d814",
            "622bb6e22453446b9d23fcc047c5d350",
            "eb50c2164a194d1abf75cb4aba79557e",
            "8ae94de70bfe4ba3bca86071f6d58ef3",
            "7fc38b477af94798b096b819a155e2d4"
          ]
        },
        "id": "rwl9iyxOFvf4",
        "outputId": "3426fbba-e9cf-4f00-f973-053f69f61b88"
      },
      "source": [
        "# map data to input\n",
        "def generate_input_sequence(rating, category, title, description):\n",
        "    rating_sequence = '{} {}'.format(RATING_TOKEN, rating)\n",
        "    category_sequence = '{} {}'.format(CATEGORY_TOKEN, category)\n",
        "    title_sequence = '{} {}'.format(TITLE_TOKEN, title)\n",
        "    description_sequence = '{} {}'.format(DESCRIPTION_TOKEN, description)\n",
        "    return ' '.join([rating_sequence, category_sequence, title_sequence, description_sequence])\n",
        "\n",
        "# tokenize input for encoder\n",
        "def preprocess_encoder_input(sequence):\n",
        "    tokenized_input = encoder_tokenizer.tokenize(sequence)\n",
        "    encoded_input = encoder_tokenizer.convert_tokens_to_ids(tokenized_input)\n",
        "    prepared_input = encoder_tokenizer.prepare_for_model(\n",
        "        encoded_input,\n",
        "        max_length=ENCODER_MAX_LENGTH,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "    return prepared_input['input_ids'], prepared_input['attention_mask']\n",
        "\n",
        "# tokenize input for decoder\n",
        "def preprocess_decoder_input(sequence):\n",
        "    tokenized_input = decoder_tokenizer.tokenize(sequence)\n",
        "    encoded_input = decoder_tokenizer.convert_tokens_to_ids(tokenized_input)\n",
        "    prepared_input = decoder_tokenizer.prepare_for_model(\n",
        "        encoded_input,\n",
        "        max_length=DECODER_MAX_LENGTH,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "    return prepared_input['input_ids'], prepared_input['attention_mask']\n",
        "\n",
        "model_inputs = []\n",
        "for review in tqdm(data):\n",
        "    #product_asin = review['asin']\n",
        "    #review_rating = review['overall']\n",
        "    #review_text = review.get('reviewText')\n",
        "    \n",
        "    #product = product_dataset.get(product_asin)\n",
        "    \n",
        "    # filter reviews with missing information\n",
        "    has_text_review = True #review_text is not None\n",
        "    product_exists = True #product is not None\n",
        "    product_has_description = True #product_exists and product['description'] is not []\n",
        "    \n",
        "\n",
        "    if has_text_review and product_has_description:\n",
        "        \n",
        "        product_combined = generate_input_sequence(\n",
        "            \" \", #review_rating,\n",
        "            \" \", #product['category'],\n",
        "            \" \", #product['title'],\n",
        "            \" \", #product['description'],\n",
        "        )\n",
        "        \n",
        "        input_ids, attention_mask = preprocess_encoder_input(product_combined)\n",
        "        decoder_input_ids, decoder_attention_mask = preprocess_decoder_input(review) #review_text\n",
        "        \n",
        "        # mask pad tokens in label\n",
        "        labels = []\n",
        "        for index in decoder_input_ids:\n",
        "            label = index\n",
        "            if index is decoder_tokenizer.pad_token_id:\n",
        "                label = LABEL_MASK_TOKEN_ID\n",
        "            labels.append(label)\n",
        "    \n",
        "        model_inputs.append({\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'decoder_input_ids': decoder_input_ids,\n",
        "            'decoder_attention_mask': decoder_attention_mask,\n",
        "            'labels': labels,\n",
        "        })"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0fa507e20dc45c6a9f710af204ff053",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=15999.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJbRwF1uFvf6"
      },
      "source": [
        "Split into training and testing inputs. Initialize datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgeYAGihFvf7",
        "scrolled": true,
        "outputId": "acd76da3-c1e1-4ae6-ee04-b58a83a5257b"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -mpip install datasets==1.0.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets==1.0.2 in /usr/local/lib/python3.6/dist-packages (1.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (1.1.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (4.41.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (1.18.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets==1.0.2) (0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.0.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.0.2) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.0.2) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.0.2) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wMy-BbkFvf7",
        "scrolled": true
      },
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "data_frame = pd.DataFrame.from_records(model_inputs)\n",
        "dataset = Dataset.from_pandas(data_frame).select(range(SAMPLE_SIZE))\n",
        "dataset.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        ")\n",
        "dataset_dict = dataset.train_test_split()\n",
        "train_dataset = dataset_dict['train']\n",
        "test_dataset = dataset_dict['test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43wUMH4VFvf7"
      },
      "source": [
        "Install seq2seq trainer prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApR_3c4mFvf7",
        "scrolled": true,
        "outputId": "def09388-563e-4374-e47e-efadc97432d0"
      },
      "source": [
        "# !pip install rouge_score\n",
        "!{sys.executable} -mpip install rouge_score\n",
        "# !rm seq2seq_trainer.py\n",
        "# !rm seq2seq_training_args.py\n",
        "# !wget https://raw.githubusercontent.com/huggingface/transformers/v3.5.1/examples/seq2seq/seq2seq_trainer.py\n",
        "# !wget https://raw.githubusercontent.com/huggingface/transformers/v3.5.1/examples/seq2seq/seq2seq_training_args.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.6/dist-packages (0.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from rouge_score) (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rouge_score) (1.18.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from rouge_score) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRXmAtceFvf7"
      },
      "source": [
        "Train model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsWgDEfuFvf7"
      },
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "def compute_metrics(outputs):\n",
        "    predictions_ids = outputs.predictions\n",
        "    labels_ids = outputs.label_ids\n",
        "\n",
        "    predictions = decoder_tokenizer.batch_decode(predictions_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == LABEL_MASK_TOKEN_ID] = decoder_tokenizer.eos_token_id\n",
        "    labels = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(\n",
        "        predictions=predictions,\n",
        "        references=labels,\n",
        "        rouge_types=[\"rouge2\"]\n",
        "    )[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itxL-QIvo9ea"
      },
      "source": [
        "The next cell is taken from the transformers examples directory. Importing was broken for it and it alone for some reason, so I copied it and removed things until it worked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrAre_ks3o-e"
      },
      "source": [
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DistributedSampler, RandomSampler\n",
        "\n",
        "from transformers import PreTrainedModel, Trainer, logging\n",
        "from transformers.file_utils import is_torch_tpu_available\n",
        "from transformers.models.fsmt.configuration_fsmt import FSMTConfig\n",
        "from transformers.optimization import (\n",
        "    Adafactor,\n",
        "    AdamW,\n",
        "    get_constant_schedule,\n",
        "    get_constant_schedule_with_warmup,\n",
        "    get_cosine_schedule_with_warmup,\n",
        "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    get_polynomial_decay_schedule_with_warmup,\n",
        ")\n",
        "from transformers.trainer_pt_utils import get_tpu_sampler\n",
        "#from transformers.training_args import ParallelMode\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "arg_to_scheduler = {\n",
        "    \"linear\": get_linear_schedule_with_warmup,\n",
        "    \"cosine\": get_cosine_schedule_with_warmup,\n",
        "    \"cosine_w_restarts\": get_cosine_with_hard_restarts_schedule_with_warmup,\n",
        "    \"polynomial\": get_polynomial_decay_schedule_with_warmup,\n",
        "    \"constant\": get_constant_schedule,\n",
        "    \"constant_w_warmup\": get_constant_schedule_with_warmup,\n",
        "}\n",
        "\n",
        "\n",
        "class Seq2SeqTrainer(Trainer):\n",
        "    def __init__(self, config=None, data_args=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        if config is None:\n",
        "            assert isinstance(\n",
        "                self.model, PreTrainedModel\n",
        "            ), f\"If no `config` is passed the model to be trained has to be of type `PreTrainedModel`, but is {self.model.__class__}\"\n",
        "            self.config = self._actual_model(self.model).config\n",
        "        else:\n",
        "            self.config = config\n",
        "\n",
        "        self.data_args = data_args\n",
        "        self.vocab_size = self.config.tgt_vocab_size if isinstance(self.config, FSMTConfig) else self.config.vocab_size\n",
        "\n",
        "        if self.args.label_smoothing != 0 or (self.data_args is not None and self.data_args.ignore_pad_token_for_loss):\n",
        "            assert (\n",
        "                self.config.pad_token_id is not None\n",
        "            ), \"Make sure that `config.pad_token_id` is correcly defined when ignoring `pad_token` for loss calculation or doing label smoothing.\"\n",
        "\n",
        "        if self.config.pad_token_id is None and self.config.eos_token_id is not None:\n",
        "            logger.warn(\n",
        "                f\"The `config.pad_token_id` is `None`. Using `config.eos_token_id` = {self.config.eos_token_id} for padding..\"\n",
        "            )\n",
        "\n",
        "        if self.args.label_smoothing == 0:\n",
        "            self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=self.config.pad_token_id)\n",
        "        else:\n",
        "            # dynamically import label_smoothed_nll_loss\n",
        "            from utils import label_smoothed_nll_loss\n",
        "\n",
        "            self.loss_fn = label_smoothed_nll_loss\n",
        "\n",
        "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
        "        \"\"\"\n",
        "        Setup the optimizer and the learning rate scheduler.\n",
        "        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
        "        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
        "        \"\"\"\n",
        "        if self.optimizer is None:\n",
        "            no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "            optimizer_grouped_parameters = [\n",
        "                {\n",
        "                    \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                    \"weight_decay\": self.args.weight_decay,\n",
        "                },\n",
        "                {\n",
        "                    \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                    \"weight_decay\": 0.0,\n",
        "                },\n",
        "            ]\n",
        "            if self.args.adafactor:\n",
        "                self.optimizer = Adafactor(\n",
        "                    optimizer_grouped_parameters,\n",
        "                    lr=self.args.learning_rate,\n",
        "                    scale_parameter=False,\n",
        "                    relative_step=False,\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                self.optimizer = AdamW(\n",
        "                    optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon\n",
        "                )\n",
        "\n",
        "        if self.lr_scheduler is None:\n",
        "            self.lr_scheduler = self._get_lr_scheduler(num_training_steps)\n",
        "        else:  # ignoring --lr_scheduler\n",
        "            logger.warn(\"scheduler is passed to `Seq2SeqTrainer`, `--lr_scheduler` arg is ignored.\")\n",
        "\n",
        "    def _get_lr_scheduler(self, num_training_steps):\n",
        "        schedule_func = arg_to_scheduler[self.args.lr_scheduler]\n",
        "        if self.args.lr_scheduler == \"constant\":\n",
        "            scheduler = schedule_func(self.optimizer)\n",
        "        elif self.args.lr_scheduler == \"constant_w_warmup\":\n",
        "            scheduler = schedule_func(self.optimizer, num_warmup_steps=self.args.warmup_steps)\n",
        "        else:\n",
        "            scheduler = schedule_func(\n",
        "                self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps\n",
        "            )\n",
        "        return scheduler\n",
        "\n",
        "    def _get_train_sampler(self) -> Optional[torch.utils.data.sampler.Sampler]:\n",
        "        if isinstance(self.train_dataset, torch.utils.data.IterableDataset):\n",
        "            return None\n",
        "        elif is_torch_tpu_available():\n",
        "            return get_tpu_sampler(self.train_dataset)\n",
        "        else:\n",
        "            if self.args.sortish_sampler:\n",
        "                self.train_dataset.make_sortish_sampler(\n",
        "                    self.args.per_device_train_batch_size,\n",
        "                    distributed=(self.args.parallel_mode == ParallelMode.DISTRIBUTED),\n",
        "                )\n",
        "\n",
        "            return (\n",
        "                RandomSampler(self.train_dataset)\n",
        "                if self.args.local_rank == -1\n",
        "                else DistributedSampler(self.train_dataset)\n",
        "            )\n",
        "\n",
        "    def _compute_loss(self, model, inputs, labels):\n",
        "        if self.args.label_smoothing == 0:\n",
        "            if self.data_args is not None and self.data_args.ignore_pad_token_for_loss:\n",
        "                # force training to ignore pad token\n",
        "                logits = model(**inputs, use_cache=False)[0]\n",
        "                loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
        "            else:\n",
        "                # compute usual loss via models\n",
        "                loss, logits = model(**inputs, labels=labels, use_cache=False)[:2]\n",
        "        else:\n",
        "            # compute label smoothed loss\n",
        "            logits = model(**inputs, use_cache=False)[0]\n",
        "            lprobs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "            loss, _ = self.loss_fn(lprobs, labels, self.args.label_smoothing, ignore_index=self.config.pad_token_id)\n",
        "        return loss, logits\n",
        "\n",
        "    def compute_loss(self, model, inputs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        loss, _ = self._compute_loss(model, inputs, labels)\n",
        "        return loss\n",
        "\n",
        "    def prediction_step(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
        "        prediction_loss_only: bool,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Perform an evaluation step on :obj:`model` using obj:`inputs`.\n",
        "        Subclass and override to inject custom behavior.\n",
        "        Args:\n",
        "            model (:obj:`nn.Module`):\n",
        "                The model to evaluate.\n",
        "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
        "                The inputs and targets of the model.\n",
        "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
        "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
        "            prediction_loss_only (:obj:`bool`):\n",
        "                Whether or not to return the loss only.\n",
        "        Return:\n",
        "            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "            A tuple with the loss, logits and labels (each being optional).\n",
        "        \"\"\"\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "        gen_kwargs = {\n",
        "            \"max_length\": self.data_args.val_max_target_length\n",
        "            if self.data_args is not None\n",
        "            else self.config.max_length,\n",
        "            \"num_beams\": self.data_args.eval_beams if self.data_args is not None else self.config.num_beams,\n",
        "        }\n",
        "\n",
        "        if self.args.predict_with_generate and not self.args.prediction_loss_only:\n",
        "            generated_tokens = self.model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                **gen_kwargs,\n",
        "            )\n",
        "            # in case the batch is shorter than max length, the output should be padded\n",
        "            if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n",
        "                generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n",
        "\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        with torch.no_grad():\n",
        "            # compute loss on predict data\n",
        "            loss, logits = self._compute_loss(model, inputs, labels)\n",
        "\n",
        "        loss = loss.mean().detach()\n",
        "        if self.args.prediction_loss_only:\n",
        "            return (loss, None, None)\n",
        "\n",
        "        logits = generated_tokens if self.args.predict_with_generate else logits\n",
        "\n",
        "        if labels.shape[-1] < gen_kwargs[\"max_length\"]:\n",
        "            labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n",
        "\n",
        "        return (loss, logits, labels)\n",
        "\n",
        "    def _pad_tensors_to_max_len(self, tensor, max_length):\n",
        "        # If PAD token is not defined at least EOS token has to be defined\n",
        "        pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else self.config.eos_token_id\n",
        "\n",
        "        if pad_token_id is None:\n",
        "            raise ValueError(\n",
        "                f\"Make sure that either `config.pad_token_id` or `config.eos_token_id` is defined if tensor has to be padded to `max_length`={max_length}\"\n",
        "            )\n",
        "\n",
        "        padded_tensor = pad_token_id * torch.ones(\n",
        "            (tensor.shape[0], max_length), dtype=tensor.dtype, device=tensor.device\n",
        "        )\n",
        "        padded_tensor[:, : tensor.shape[-1]] = tensor\n",
        "        return padded_tensor\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "#from seq2seq_trainer import arg_to_scheduler\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Seq2SeqTrainingArguments(TrainingArguments):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        label_smoothing (:obj:`float`, `optional`, defaults to 0):\n",
        "            The label smoothing epsilon to apply (if not zero).\n",
        "        sortish_sampler (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether to SortishSamler or not. It sorts the inputs according to lenghts in-order to minimizing the padding size.\n",
        "        predict_with_generate (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether to use generate to calculate generative metrics (ROUGE, BLEU).\n",
        "    \"\"\"\n",
        "\n",
        "    label_smoothing: Optional[float] = field(\n",
        "        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (if not zero).\"}\n",
        "    )\n",
        "    sortish_sampler: bool = field(default=False, metadata={\"help\": \"Whether to SortishSamler or not.\"})\n",
        "    predict_with_generate: bool = field(\n",
        "        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n",
        "    )\n",
        "    adafactor: bool = field(default=False, metadata={\"help\": \"whether to use adafactor\"})\n",
        "    encoder_layerdrop: Optional[float] = field(\n",
        "        default=None, metadata={\"help\": \"Encoder layer dropout probability. Goes into model.config.\"}\n",
        "    )\n",
        "    decoder_layerdrop: Optional[float] = field(\n",
        "        default=None, metadata={\"help\": \"Decoder layer dropout probability. Goes into model.config.\"}\n",
        "    )\n",
        "    dropout: Optional[float] = field(default=None, metadata={\"help\": \"Dropout probability. Goes into model.config.\"})\n",
        "    attention_dropout: Optional[float] = field(\n",
        "        default=None, metadata={\"help\": \"Attention dropout probability. Goes into model.config.\"}\n",
        "    )\n",
        "    lr_scheduler: Optional[str] = field(\n",
        "        default=\"linear\",\n",
        "        metadata={\"help\": f\"Which lr scheduler to use. Selected in {sorted(arg_to_scheduler.keys())}\"},\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PAIuhVin1nE",
        "outputId": "c3430116-001c-4dc1-f3e1-729a6a395c95"
      },
      "source": [
        "%debug"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py\u001b[0m(880)\u001b[0;36m<genexpr>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    878 \u001b[0;31m    \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    879 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 880 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpy_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    881 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    882 \u001b[0;31m    \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> print(py_dict)\n",
            "*** NameError: name 'py_dict' is not defined\n",
            "ipdb> print py_dict\n",
            "*** SyntaxError: Missing parentheses in call to 'print'. Did you mean print(py_dict)?\n",
            "ipdb> print(py_dict)\n",
            "*** NameError: name 'py_dict' is not defined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IrvyX5NqFvf7",
        "scrolled": true,
        "outputId": "3a19aa4c-e52f-4ede-e19e-a7aa5d1fd9ff"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    output_dir=os.path.join(MODEL_TEMP_PATH, 'results'),\n",
        "    overwrite_output_dir=True,\n",
        "    #save_steps=10,\n",
        "    save_steps=500,\n",
        "    evaluation_strategy='steps',\n",
        "    #eval_steps=4,\n",
        "    eval_steps=7500,\n",
        "    logging_dir=os.path.join(MODEL_TEMP_PATH, 'runs'),\n",
        "    #logging_steps=2,\n",
        "    logging_steps=1000,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    fp16=True,\n",
        "    warmup_steps=2000,\n",
        "    save_total_limit=3,\n",
        ")\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-94369f7a4fdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_all_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0mformat_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         )\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, format_type, format_columns, output_all_columns, format_kwargs)\u001b[0m\n\u001b[1;32m    944\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                     outputs = self._unnest(\n\u001b[0;32m--> 946\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes_mapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_types_mapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m                     )\n\u001b[1;32m    948\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_unnest\u001b[0;34m(py_dict)\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpy_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpy_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YXDawD2Fvf8"
      },
      "source": [
        "Save model to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49OzXK9iFvf8"
      },
      "source": [
        "model.save_pretrained(MODEL_SAVE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB5z9R4EFvf8"
      },
      "source": [
        "Load model from disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps0E9QGoFvf8"
      },
      "source": [
        "model = EncoderDecoderModel.from_pretrained(MODEL_SAVE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rwPkWLYFvf8"
      },
      "source": [
        "Predict from input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suePi9ZfFvf8"
      },
      "source": [
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "product = list(product_dataset.values())[0]\n",
        "product_category = product['category']\n",
        "product_title = product['title']\n",
        "product_description = product['description']\n",
        "product_combined = generate_input_sequence(5, product_category, product_title, product_description)\n",
        "\"\"\"\n",
        "\n",
        "input_ids, _ = preprocess_encoder_input(product_combined)\n",
        "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
        "\n",
        "output_ids = model.generate(\n",
        "    input_ids,\n",
        "    decoder_start_token_id=model.config.decoder.pad_token_id,\n",
        "    temperature=1.3,\n",
        "    top_k=9,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.4\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1HKV0fnFvf9"
      },
      "source": [
        "for i in output_ids[:10]:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWJb5hSXFvf9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}